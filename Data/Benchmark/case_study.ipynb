{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synkit.IO import load_database\n",
    "\n",
    "data = load_database('./data_aam.json.gz')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Chem package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Standardization and Canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from synkit.IO.data_process import TqdmJoblib\n",
    "from synkit.Chem.Reaction.standardize import Standardize\n",
    "from synkit.Chem.Reaction.canon_rsmi import CanonRSMI\n",
    "\n",
    "def safe_std_canon(aam: Any) -> Optional[str]:\n",
    "    if aam is None:\n",
    "        return None\n",
    "    try:\n",
    "        std = Standardize()\n",
    "        canon = CanonRSMI(wl_iterations=4)\n",
    "        aam_std = std.fit(aam, remove_aam=False)\n",
    "        canon_out = canon.canonicalise(aam_std)\n",
    "        return getattr(canon_out, \"canonical_rsmi\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def process_item(pair: Tuple[int, Dict[str, Any]], keys: Tuple[str, ...]) -> Dict[str, Any]:\n",
    "    idx, item = pair\n",
    "    out = item.copy()\n",
    "    for k in keys:\n",
    "        val = item.get(k)\n",
    "        out[k] = safe_std_canon(val)\n",
    "    return out\n",
    "\n",
    "def parallel_std_canon_tqdm(\n",
    "    data: List[Dict[str, Any]],\n",
    "    n_jobs: int = -1,\n",
    "    keys: Tuple[str, ...] = (\"rxn_mapper\", \"graphormer\", \"local_mapper\"),\n",
    "    prefer_backend: str = \"loky\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    enumerated = list(enumerate(data))\n",
    "    total = len(enumerated)\n",
    "\n",
    "    def _proc(pair):\n",
    "        return process_item(pair, keys)\n",
    "\n",
    "    try:\n",
    "        with TqdmJoblib(tqdm(total=total, desc=\"std+canon\", unit=\"it\")):\n",
    "            result = Parallel(n_jobs=n_jobs, backend=prefer_backend)(\n",
    "                delayed(_proc)(pair) for pair in enumerated\n",
    "            )\n",
    "    except Exception:\n",
    "        # retry with threads if processes/pickling fail\n",
    "        with TqdmJoblib(tqdm(total=total, desc=\"std+canon\", unit=\"it\")):\n",
    "            result = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "                delayed(_proc)(pair) for pair in enumerated\n",
    "            )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = parallel_std_canon_tqdm(data, n_jobs=4)\n",
    "data[:] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 2. Ensemble AAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Any, Optional, Union\n",
    "\n",
    "def extract_aam(\n",
    "    data: List[Dict[str, Any]],\n",
    "    keys: Tuple[str, str, str] = (\"rxn_mapper\", \"graphormer\", \"local_mapper\"),\n",
    "    strip: bool = True,\n",
    "    require_not_none: bool = True,\n",
    "    return_indices: bool = False,\n",
    "    inplace: bool = False,\n",
    ") -> Union[\n",
    "    Tuple[List[Dict[str, Any]], List[Dict[str, Any]]],\n",
    "    Tuple[List[Dict[str, Any]], List[int], List[Dict[str, Any]], List[int]],\n",
    "]:\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    idxs: List[int] = []\n",
    "    non_out: List[Dict[str, Any]] = []\n",
    "    non_idxs: List[int] = []\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        v0 = item.get(keys[0])\n",
    "        v1 = item.get(keys[1])\n",
    "        v2 = item.get(keys[2])\n",
    "\n",
    "        if strip:\n",
    "            v0 = v0.strip() if isinstance(v0, str) else v0\n",
    "            v1 = v1.strip() if isinstance(v1, str) else v1\n",
    "            v2 = v2.strip() if isinstance(v2, str) else v2\n",
    "\n",
    "        if require_not_none and (v0 is None or v1 is None or v2 is None):\n",
    "            non_out.append(item)\n",
    "            non_idxs.append(i)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            equal = (v0 == v1 == v2)\n",
    "        except Exception:\n",
    "            equal = False\n",
    "\n",
    "        if equal:\n",
    "            aam_val = v0\n",
    "            new_item = item.copy()\n",
    "            for k in keys:\n",
    "                new_item.pop(k, None)\n",
    "            new_item[\"aam\"] = aam_val\n",
    "            out.append(new_item)\n",
    "            idxs.append(i)\n",
    "        else:\n",
    "            non_out.append(item)\n",
    "            non_idxs.append(i)\n",
    "\n",
    "    if inplace:\n",
    "        data[:] = out\n",
    "\n",
    "    if return_indices:\n",
    "        return out, idxs, non_out, non_idxs\n",
    "    return out, non_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, non_matches = extract_aam(data)\n",
    "print(len(matches), len(non_matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Filter by isomorphism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from synkit.Chem.Reaction.aam_validator import AAMValidator\n",
    "\n",
    "def resolve_non_matches(\n",
    "    non_matches: List[Dict[str, Any]],\n",
    "    validator: Optional[AAMValidator] = None,\n",
    "    keys: Tuple[str, str, str] = (\"rxn_mapper\", \"graphormer\", \"local_mapper\"),\n",
    "    strip: bool = True,\n",
    "    allow_interrupt: bool = True,\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    if validator is None:\n",
    "        validator = AAMValidator()\n",
    "\n",
    "    resolved: List[Dict[str, Any]] = []\n",
    "    remaining: List[Dict[str, Any]] = []\n",
    "\n",
    "    key_rxn, key_graph, key_local = keys\n",
    "\n",
    "    for item in non_matches:\n",
    "        try:\n",
    "            v_rxn = item.get(key_rxn)\n",
    "            v_graph = item.get(key_graph)\n",
    "            v_local = item.get(key_local)\n",
    "\n",
    "            if strip:\n",
    "                v_rxn = v_rxn.strip() if isinstance(v_rxn, str) else v_rxn\n",
    "                v_graph = v_graph.strip() if isinstance(v_graph, str) else v_graph\n",
    "                v_local = v_local.strip() if isinstance(v_local, str) else v_local\n",
    "\n",
    "            if v_rxn is None or v_graph is None or v_local is None:\n",
    "                remaining.append(item)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ok_local = bool(validator.smiles_check(v_rxn, v_local))\n",
    "            except KeyboardInterrupt:\n",
    "                if allow_interrupt:\n",
    "                    raise\n",
    "                ok_local = False\n",
    "            except Exception:\n",
    "                ok_local = False\n",
    "\n",
    "            try:\n",
    "                ok_graph = bool(validator.smiles_check(v_rxn, v_graph))\n",
    "            except KeyboardInterrupt:\n",
    "                if allow_interrupt:\n",
    "                    raise\n",
    "                ok_graph = False\n",
    "            except Exception:\n",
    "                ok_graph = False\n",
    "\n",
    "            if ok_local and ok_graph:\n",
    "                new_item = item.copy()\n",
    "                new_item.pop(key_rxn, None)\n",
    "                new_item.pop(key_graph, None)\n",
    "                new_item.pop(key_local, None)\n",
    "                new_item[\"aam\"] = v_rxn\n",
    "                resolved.append(new_item)\n",
    "            else:\n",
    "                remaining.append(item)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception:\n",
    "            remaining.append(item)\n",
    "\n",
    "    return resolved, remaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved, still_unresolved = resolve_non_matches(non_matches)\n",
    "matches.extend(resolved)\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(matches)\n",
    "data.drop_duplicates(subset='aam', inplace=True)\n",
    "data = data.to_dict('records')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. IO\n",
    "Convert to ITS and reaction center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synkit.IO import rsmi_to_its\n",
    "for value in data:\n",
    "    value['ITS'] = rsmi_to_its(value['aam'], core=False)\n",
    "    value['RC'] = rsmi_to_its(value['aam'], core=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Graph module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synkit.Graph.Feature.wl_hash import WLHash\n",
    "from synkit.Graph.Matcher.graph_cluster import GraphCluster\n",
    "for value in data:\n",
    "    value['wl'] = WLHash().weisfeiler_lehman_graph_hash(value['RC'])\n",
    "cls = GraphCluster()\n",
    "result = cls.fit(data, rule_key='RC', attribute_key='wl') # quick filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synkit.Utils.utils import stratified_random_sample\n",
    "from synkit.Rule.syn_rule import SynRule\n",
    "rule = stratified_random_sample(result, 'class', 1)\n",
    "rule = [value['ITS'] for value in rule]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Rule application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = result[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "import copy\n",
    "import logging\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import concurrent.futures\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm  # optional\n",
    "except Exception:\n",
    "    tqdm = None  # type: ignore\n",
    "\n",
    "from synkit.IO.chem_converter import rsmi_to_graph, rsmi_to_its\n",
    "from synkit.Graph.ITS.its_decompose import get_rc\n",
    "from synkit.Chem.Reaction.standardize import Standardize\n",
    "from synkit.Synthesis.Reactor.syn_reactor import SynReactor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def process_reaction_entry(\n",
    "    entry: Dict[str, Any],\n",
    "    smart_key: str = \"aam\",\n",
    "    explicit_h: bool = False,\n",
    "    invert: bool = False,\n",
    "    implicit_temp: bool = True,\n",
    "    strategy: str = \"all\",\n",
    "    embed_pre_filter: bool = False,\n",
    "    return_reactor: bool = False,\n",
    ") -> Union[List[str], Dict[str, Any]]:\n",
    "    if smart_key not in entry:\n",
    "        raise KeyError(f\"Entry missing required key '{smart_key}'\")\n",
    "    smi = entry[smart_key]\n",
    "    if not isinstance(smi, str) or not smi.strip():\n",
    "        raise ValueError(f\"Value for '{smart_key}' must be a non-empty SMILES string\")\n",
    "    try:\n",
    "        rsmi = Standardize().fit(smi, remove_aam=True)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Standardization failed for SMILES: %s\", smi)\n",
    "        raise RuntimeError(\"Standardize().fit(...) failed\") from e\n",
    "    try:\n",
    "        left, right = rsmi_to_graph(rsmi, drop_non_aam=False, use_index_as_atom_map=False)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"rsmi_to_graph failed for RSMS: %s\", rsmi)\n",
    "        raise RuntimeError(\"rsmi_to_graph(...) failed\") from e\n",
    "    try:\n",
    "        its = rsmi_to_its(smi)\n",
    "        rc = get_rc(its)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"ITS decomposition or get_rc failed for SMILES: %s\", smi)\n",
    "        raise RuntimeError(\"rsmi_to_its(...) or get_rc(...) failed\") from e\n",
    "    if rc is None:\n",
    "        raise ValueError(\"Reaction center (rc) could not be determined from ITS\")\n",
    "    substrate = right if invert else left\n",
    "    try:\n",
    "        reactor = SynReactor(\n",
    "            substrate,\n",
    "            rc,\n",
    "            explicit_h=explicit_h,\n",
    "            invert=invert,\n",
    "            strategy=strategy,\n",
    "            embed_pre_filter=embed_pre_filter,\n",
    "            implicit_temp=implicit_temp,\n",
    "            automorphism=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.exception(\"SynReactor initialization failed\")\n",
    "        raise RuntimeError(\"SynReactor(...) failed\") from e\n",
    "    smarts = getattr(reactor, \"smarts\", None)\n",
    "    if smarts is None:\n",
    "        try:\n",
    "            smarts = list(reactor.get_smarts())\n",
    "        except Exception:\n",
    "            logger.error(\"SynReactor returned no 'smarts' attribute or get_smarts method\")\n",
    "            raise RuntimeError(\"Could not obtain SMARTS from SynReactor instance\")\n",
    "    if return_reactor:\n",
    "        return {\"smarts\": smarts, \"reactor\": reactor}\n",
    "    return smarts\n",
    "\n",
    "\n",
    "def dict_process(*args, **kwargs):\n",
    "    if \"strat\" in kwargs and \"strategy\" not in kwargs:\n",
    "        kwargs[\"strategy\"] = kwargs.pop(\"strat\")\n",
    "    return process_reaction_entry(*args, **kwargs)\n",
    "\n",
    "\n",
    "def _safe_process_pair(entry: Dict[str, Any], *, process_fn, catch_traceback: bool = True) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    try:\n",
    "        fw = process_fn(entry, invert=False)\n",
    "        out[\"fw\"] = fw\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Forward processing failed for entry (preview): %r\", entry.get(\"id\") or entry.get(\"aam\"))\n",
    "        out[\"fw_error\"] = str(e)\n",
    "        if catch_traceback:\n",
    "            out[\"fw_traceback\"] = traceback.format_exc()\n",
    "    try:\n",
    "        bw = process_fn(entry, invert=True)\n",
    "        out[\"bw\"] = bw\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Backward processing failed for entry (preview): %r\", entry.get(\"id\") or entry.get(\"aam\"))\n",
    "        out[\"bw_error\"] = str(e)\n",
    "        if catch_traceback:\n",
    "            out[\"bw_traceback\"] = traceback.format_exc()\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_entries_parallel(\n",
    "    entries,\n",
    "    *,\n",
    "    process_fn,\n",
    "    n_jobs: int = -1,\n",
    "    backend: str = \"loky\",\n",
    "    batch_size: Optional[int] = None,\n",
    "    mutate_inplace: bool = True,\n",
    "    show_progress: bool = False,\n",
    "    use_threads_for_progress: bool = True,\n",
    "):\n",
    "    entries_list = list(entries)\n",
    "    if not entries_list:\n",
    "        return []\n",
    "    if n_jobs is None or n_jobs == 0:\n",
    "        n_jobs = 1\n",
    "    if n_jobs < 0:\n",
    "        n_workers = os.cpu_count() or 1\n",
    "    else:\n",
    "        n_workers = n_jobs\n",
    "    def _worker(e: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        base = e if mutate_inplace else copy.deepcopy(e)\n",
    "        result = _safe_process_pair(e, process_fn=process_fn)\n",
    "        base.update(result)\n",
    "        return base\n",
    "    if show_progress and tqdm is not None:\n",
    "        executor_cls = concurrent.futures.ThreadPoolExecutor if use_threads_for_progress else concurrent.futures.ProcessPoolExecutor\n",
    "        results = []\n",
    "        try:\n",
    "            with executor_cls(max_workers=n_workers) as ex:\n",
    "                futures = [ex.submit(_worker, e) for e in entries_list]\n",
    "                for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"process_entries\"):\n",
    "                    try:\n",
    "                        results.append(fut.result())\n",
    "                    except Exception:\n",
    "                        tb = traceback.format_exc()\n",
    "                        logger.exception(\"Worker raised unexpectedly: %s\", tb)\n",
    "                        results.append({\"_worker_error\": tb})\n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(\"KeyboardInterrupt received â€” attempting to cancel running futures\")\n",
    "            raise\n",
    "        return results\n",
    "    try:\n",
    "        parallel = Parallel(n_jobs=n_jobs, backend=backend, batch_size=batch_size)\n",
    "        results = parallel(delayed(_worker)(e) for e in entries_list)\n",
    "        return results\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"KeyboardInterrupt propagated from joblib.Parallel\")\n",
    "        raise\n",
    "    except Exception:\n",
    "        logger.exception(\"joblib.Parallel failed; falling back to sequential processing\")\n",
    "        out = []\n",
    "        for e in entries_list:\n",
    "            out.append(_worker(e))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "_prev_logging_state: Dict = {}\n",
    "\n",
    "def force_disable_logging() -> None:\n",
    "    \"\"\"\n",
    "    Forcefully silence Python logging (best-effort). Call restore_logging() to bring things back.\n",
    "    \"\"\"\n",
    "    global _prev_logging_state\n",
    "    if _prev_logging_state:\n",
    "        return  # already disabled\n",
    "\n",
    "    manager = logging.root.manager\n",
    "    _prev_logging_state[\"disabled\"] = manager.disable\n",
    "    _prev_logging_state[\"root_level\"] = logging.getLogger().level\n",
    "    _prev_logging_state[\"loggers\"] = {}\n",
    "\n",
    "    # raise level and remove handlers for all known loggers (best-effort)\n",
    "    for name, obj in list(manager.loggerDict.items()):\n",
    "        if isinstance(obj, logging.Logger):\n",
    "            logger: logging.Logger = obj\n",
    "            _prev_logging_state[\"loggers\"][name] = (\n",
    "                logger.level,\n",
    "                logger.propagate,\n",
    "                list(logger.handlers),\n",
    "            )\n",
    "            try:\n",
    "                logger.handlers = []\n",
    "            except Exception:\n",
    "                # some loggers may not allow direct reassignment; ignore\n",
    "                pass\n",
    "            try:\n",
    "                logger.setLevel(logging.CRITICAL + 10)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                logger.propagate = False\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # also silence the root logger\n",
    "    try:\n",
    "        root = logging.getLogger()\n",
    "        _prev_logging_state[\"root_handlers\"] = list(root.handlers)\n",
    "        root.handlers = []\n",
    "        root.setLevel(logging.CRITICAL + 10)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # globally disable logging up to and including CRITICAL (effectively all levels)\n",
    "    logging.disable(logging.CRITICAL)\n",
    "\n",
    "\n",
    "def restore_logging() -> None:\n",
    "    \"\"\"\n",
    "    Restore logging state saved by force_disable_logging(). Safe to call even if nothing was saved.\n",
    "    \"\"\"\n",
    "    global _prev_logging_state\n",
    "    if not _prev_logging_state:\n",
    "        return\n",
    "\n",
    "    manager = logging.root.manager\n",
    "\n",
    "    # restore global disable\n",
    "    try:\n",
    "        manager.disable = _prev_logging_state.get(\"disabled\", logging.NOTSET)\n",
    "        logging.disable(manager.disable)\n",
    "    except Exception:\n",
    "        logging.disable(logging.NOTSET)\n",
    "\n",
    "    # restore root logger\n",
    "    try:\n",
    "        root = logging.getLogger()\n",
    "        root.handlers = _prev_logging_state.get(\"root_handlers\", [])\n",
    "        root.setLevel(_prev_logging_state.get(\"root_level\", logging.NOTSET))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # restore individual loggers\n",
    "    for name, (level, propagate, handlers) in _prev_logging_state.get(\"loggers\", {}).items():\n",
    "        try:\n",
    "            logger = logging.getLogger(name)\n",
    "            logger.handlers = handlers\n",
    "            logger.setLevel(level)\n",
    "            logger.propagate = propagate\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    _prev_logging_state.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_disable_logging()\n",
    "processed = process_entries_parallel(\n",
    "    df[:],\n",
    "    process_fn=process_reaction_entry,\n",
    "    n_jobs=4,\n",
    "    backend=\"loky\",\n",
    "    mutate_inplace=True,        # original dicts updated\n",
    "        show_progress=True,        # set True to use concurrent.futures + tqdm if you'd like a progress bar\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
